# PaperMind：多模态学术论文 RAG 智能体系统汇总文档

## 1. 项目概述与核心目标

**PaperMind** 是一个针对学术科研场景设计的集成化 RAG 系统。该项目通过三个各具特色的智能体（Agent），解决了从**多模态知识库深度问答**、**个人文档即时解析**到**全球学术资源动态检索**的全链路需求。

*   **多模态问答智能体 (Agent 1)**：专注于预构建库的高质量图文协同推理。
*   **即时文档问答智能体 (Agent 2)**：侧重于用户上传文档的快速响应与轻量化处理。
*   **自检索论文推荐智能体 (Agent 3)**：利用外部 API 实现自主的文献调研与深度阅读。

---

## 2. 核心智能体架构设计

### 2.1 Agent 1——多模态知识库问答系统

#### 2.1.1 核心设计理念

Agent 1 定位于**垂直领域深度问答**。它基于预构建的多模态知识库，通过 LangGraph 构建复杂的受控工作流，实现了文本与图像信息的协同检索与联合推理。

#### 2.1.2 基于 LangGraph 的状态机架构

为了解决传统链式 RAG 逻辑僵化的问题，Agent 1 采用了基于状态机的图结构：

##### 2.1.2.1 状态定义（AgentState）

系统通过 `AgentState` 维护全局状态，确保多模态信息的无损传递：

*   **`messages`**: 维护对话的短期记忆，支撑多轮上下文理解。
*   **`retrieved_images`**: 专门用于暂存检索到的图像元数据，待预处理后交由多模态模型解析。

##### 2.1.2.2 工作流节点解析

*   **Agent 节点**: 核心决策大脑，封装了 `qwen-vl` 等多模态大模型，动态判断是否需要调用工具。
*   **Tools 节点**: 执行向量检索任务。
*   **Process_Tool 节点**: 关键的工程化步骤。它将检索结果转化为模型易理解的自然语言，并执行**路径校验与绝对路径转换**，确保模型能准确读取本地图像资源。
*   **Add_Images 节点**: 将图像资源封装为 `HumanMessage` 结构，实现图文混合输入的标准化封装。

#### 2.1.3 工业级多模态检索管线

Agent 1 放弃了单一检索模式，构建了一套包含“扩展-查询-排序-重排”的完整 Pipeline：

##### 2.1.3.1 阶段一：检索增强（MQE & HyDE）

*   **多查询扩展 (MQE)**: 针对学术问题表述模糊的痛点，利用 `qwen3-14b` 并发生成多个同义查询，提升召回覆盖率。
*   **假设答案生成**: 预先生成伪答案并进行嵌入，利用“答案-答案”的语义相似性克服“问题-答案”的语义鸿沟。

##### 2.1.3.2 阶段二：混合检索与去重

系统并发访问 **Milvus Cloud**，对返回的候选集执行相似度评分排序，并设置动态截断窗口，平衡查全率与后续计算开销。

##### 2.1.3.3 阶段三：智能重排序 (Reranking)

引入 `qwen3-rerank` 模型对文本和图片分别进行语义对齐。通过重排，将最能回答用户问题的关键图文信息精准推送到上下文窗口的前部。

---

### 2.2 Agent 2——即时文档 RAG 智能体

#### 2.2.1 核心设计理念

Agent 2 专注于**用户上传场景下的时效性与准确性平衡**。针对用户临时上传的 PDF，采用“先规划、后执行”的架构，确保在极速处理的同时提供逻辑严密的回答。

#### 2.2.2 Plan-and-Execute 任务拆解框架

面对未知结构的上传文档，Agent 2 弃用了固定路径，改用动态规划模式：

*   **Plan 智能体**: 扮演“架构师”角色。它分析用户意图，生成包含 `need_retrieval`（是否检索）、`need_context`（是否需历史记录）及 `steps`（思维链步骤）的 JSON 执行方案。
*   **Execute 智能体**: 扮演“工程师”角色。严格按照计划调用底层工具，并将中间结果汇聚为具有逻辑递进感的回答。

#### 2.2.3 极速 RAG 策略：混合检索方案

为了满足即时问答的秒级响应需求，Agent 2 在技术栈上做了精简化与性能优化：

*   **混合存储**: 采用 **Milvus Lite**（向量检索）与 **SQLite**（关键词检索）的组合。
*   **RRF 融合算法**: 使用倒排排名融合算法，将语义相似度与词频匹配得分进行非线性加权，显著提升了针对专有名词或特定公式的定位精度。

#### 2.2.4 双轨记忆系统

*   **短期记忆**: 基于内存的 N 轮滑动窗口，支持上下文自动填充与手动检索补偿。
*   **长期记忆**: 引入持久化存储。利用 `(content, timestamp, keywords, is_important)` 结构化字段，配合 TTL（生存时间）管理，实现了跨会话的知识沉淀与自动清理。

---

### 2.3 Agent 3——自主论文调研助手

#### 2.3.1 核心设计理念

Agent 3 模拟了科研人员检索文献的真实逻辑，利用 **ReAct (Reasoning and Acting)** 框架，具备自主判断是否需要进一步检索或深入解析 PDF 的能力。

#### 2.3.2 基于 ReAct 的自主逻辑循环

Agent 3 能够根据初步检索的结果（如摘要信息）自主决定：

1.  当前的搜索结果是否足以回答问题？
2.  是否需要下载特定论文的全文进行深度阅读？
3.  是否需要更换关键词重新检索？

#### 2.3.3 深度解析工具链

*   **ArXiv 集成**: 封装了从关键词搜索、元数据提取到 PDF 自动化下载的完整链路。
*   **按需索引**: 系统并不会盲目处理所有论文。只有当 Agent 决定阅读某篇论文时，才会触发 PDF 切分、向量化并动态建立 **Milvus Lite** 临时集合。
*   **缓存机制**: 引入指纹校验。若同一文档已被处理，直接复用向量索引，大幅缩短二次查询的等待时间。

---

## 3. 多模态数据预处理流程 (Preprocessing)

### 3.1 核心设计理念

知识库的质量决定了 RAG 的上限。本项目构建了一套能够理解**文档空间布局**的多模态预处理流程。

### 3.2 多模态分块策略

系统不只是简单的文本切分，而是基于 **PyMuPDF** 的坐标信息实现“版式感知”：

*   **逻辑排序**: 结合页码与 Y 轴坐标，还原真实的阅读顺序。
*   **关联存储**: 每一段文本块都会记录其关联的图片路径，每一张图片也会反向索引其上下文文本。

### 3.3 数据库 Schema 设计

在 **Milvus Cloud** 中设计了双集合架构：

*   **文本集合**: 存储 1024 维 `qwen2.5-vl-embedding` 向量。
*   **图片集合**: 存储图片内容的特征向量。
*   **分区管理**: 针对不同来源的文档创建独立分区，实现数据的逻辑隔离，确保在检索时可根据文档属性快速缩小搜索范围。

---

## 4. 前后端分离工程实现

### 4.1 极简前端方案

*   **技术选型**: 采用原生 HTML/JS 配合高性能插件。
*   **渲染能力**: 集成 `marked.js`、`MathJax` 和 `highlight.js`。这确保了学术问答中常见的**复杂 LaTeX 公式**和**代码块**能够以出版级的质量呈现。
*   **交互逻辑**: 采用三列布局。左侧实时展示 Agent 的推理日志（思维链），增强了系统的可解释性与信任感。

### 4.2 高性能异步后端

*   **FastAPI 驱动**: 利用异步协程处理耗时的模型调用与向量检索任务。
*   **智能体路由**: 后端统一封装了三个 Agent 的调度接口，支持根据前端模式选择动态路由。

### 4.3 场景化功能亮点

*   **实时预览**: 内置 PDF 预览器，支持在对话的同时查阅原始文献。
*   **多模式切换**: 灵活适配“知识库问答”、“文档上传”与“学术检索”三大高频科研场景。

---

## 5. 技术优势总结

| 维度 | 技术实现 |
| :--- | :--- |
| **检索深度** | MQE, HyDE, RRF 融合, 双路重排 |
| **理解能力** | 多模态 VLM (Qwen-VL 系列), 语义块切分 |
| **工程稳定性** | LangGraph 状态机, Plan-and-Execute 框架 |
| **科研适配** | LaTeX 公式支持, ArXiv 自动调研, PDF 实时预览 |

## 6. 快速开始

**运行后端服务**

```bash
cd backend
python run_backend.py
```

**运行前端服务**

```bash
cd frontend
python -m http.server 8080 --bind 127.0.0.1
```
